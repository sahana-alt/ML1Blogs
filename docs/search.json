[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML1 Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\nrandom variable\n\n\nprobability\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nSahana Bhaskar\n\n\n\n\n\n\n  \n\n\n\n\nNavigating Linear and Non-Linear Relationships with Machine Learning\n\n\n\n\n\n\n\nregression\n\n\nsupervised learning\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nSahana Bhaskar\n\n\n\n\n\n\n  \n\n\n\n\nDeciphering Nature’s Patterns: Classification with the Iris Dataset\n\n\n\n\n\n\n\nclassification\n\n\nensemble\n\n\nsupervised learning\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nSahana Bhaskar\n\n\n\n\n\n\n  \n\n\n\n\nA Deep Dive into Data Grouping Techniques\n\n\n\n\n\n\n\nclustering\n\n\nunsupervised learning\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nSwapnil Singh\n\n\n\n\n\n\n  \n\n\n\n\nWeathering the Storm: Anomaly Detection in Meteorological Data\n\n\n\n\n\n\n\nanomaly detection\n\n\nclustering\n\n\nunsupervised learning\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nSahana Bhaskar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html",
    "href": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html",
    "title": "Deciphering Nature’s Patterns: Classification with the Iris Dataset",
    "section": "",
    "text": "In the vast and intricate domain of machine learning, classification tasks hold a place of high regard. One of the most celebrated datasets in this area is the Iris dataset, a cornerstone for budding data scientists and seasoned professionals alike. This blog post is dedicated to exploring the nuances of classifying Iris flowers using a specific machine learning technique, enriched with practical code and visual insights.\n#The Iris Dataset: A Botanical Puzzle The Iris dataset, a hallmark in the world of machine learning, presents a fascinating challenge: to classify iris flowers into one of three species (Setosa, Versicolour, and Virginica) based on four features - sepal length, sepal width, petal length, and petal width. Each species, represented by 50 samples in the dataset, has its unique combination of these features. The goal is clear: to build a model that can accurately predict the species based on these physical measurements."
  },
  {
    "objectID": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html#preparatory-steps-setting-up-the-python-environment",
    "href": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html#preparatory-steps-setting-up-the-python-environment",
    "title": "Deciphering Nature’s Patterns: Classification with the Iris Dataset",
    "section": "Preparatory Steps: Setting Up the Python Environment",
    "text": "Preparatory Steps: Setting Up the Python Environment\nThe journey begins in Python, a haven for data science enthusiasts, courtesy of its extensive libraries. We import essential packages like Pandas for data manipulation, NumPy for numerical calculations, and Seaborn and Matplotlib for visualization. Scikit-learn, a cornerstone library in Python for machine learning, brings in the necessary algorithms and tools:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, label_binarize\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.multiclass import OneVsRestClassifier\nimport seaborn as sns"
  },
  {
    "objectID": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html#diving-into-the-data-loading-and-preparing-the-iris-dataset",
    "href": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html#diving-into-the-data-loading-and-preparing-the-iris-dataset",
    "title": "Deciphering Nature’s Patterns: Classification with the Iris Dataset",
    "section": "Diving into the Data: Loading and Preparing the Iris Dataset",
    "text": "Diving into the Data: Loading and Preparing the Iris Dataset\nThe dataset is loaded, and we proceed to bifurcate it into training and testing sets. This split is critical to evaluate our model effectively, ensuring that it can generalize well to unseen data:\n\n\nCode\n# Load the dataset\niris = pd.read_csv('IRIS.csv')\niris = iris.sample(frac=1)\nX = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\ny = iris['species']\ny = label_binarize(y, classes=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n\nn_classes = y.shape[1]\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"
  },
  {
    "objectID": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html#the-heart-of-the-matter-choosing-the-k-nearest-neighbors",
    "href": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html#the-heart-of-the-matter-choosing-the-k-nearest-neighbors",
    "title": "Deciphering Nature’s Patterns: Classification with the Iris Dataset",
    "section": "The Heart of the Matter: Choosing the K-Nearest Neighbors",
    "text": "The Heart of the Matter: Choosing the K-Nearest Neighbors\nAlgorithm For our classification task, we select the K-Nearest Neighbors (KNN) algorithm. KNN is a simple yet powerful non-parametric algorithm used in classification and regression. It operates on a straightforward principle: the class of a data point is determined by the majority class among its k-nearest neighbors. The value of ‘k’ is a crucial hyperparameter and determines the number of neighbors to consider while making the classification decision. KNN is particularly appealing for its ease of understanding and implementation. It makes no underlying assumptions about the distribution of data, making it versatile across various datasets. In our case, the decision to choose KNN stems from its efficacy in handling small, clean datasets like the Iris."
  },
  {
    "objectID": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html#implementing-the-knn-classifier",
    "href": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html#implementing-the-knn-classifier",
    "title": "Deciphering Nature’s Patterns: Classification with the Iris Dataset",
    "section": "Implementing the KNN Classifier",
    "text": "Implementing the KNN Classifier\nWe use Scikit-learn’s implementation of KNN, which involves initializing the classifier, fitting it to our training data, and then using it to make predictions:\n\n\nCode\nknn = OneVsRestClassifier(KNeighborsClassifier(n_neighbors= 3))\nknn.fit(X_train, y_train)\n\nknn.score(X_test,y_test)\n\n\n0.9333333333333333"
  },
  {
    "objectID": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html#evaluating-and-visualizing-the-models-performance",
    "href": "posts/Deciphering Natures-Patterns-Classification-with-the-Iris-Dataset/index.html#evaluating-and-visualizing-the-models-performance",
    "title": "Deciphering Nature’s Patterns: Classification with the Iris Dataset",
    "section": "Evaluating and Visualizing the Model’s Performance",
    "text": "Evaluating and Visualizing the Model’s Performance\nThe final step in our journey is to evaluate and visualize the model’s performance. A confusion matrix provides a straightforward way to visualize the accuracy of the classifier, showing the correct and incorrect predictions across different classes. .The confusion matrix (cm) is calculated from the predictions (y_pred) and the true labels (y_test). The ROC curve, in particular, provides a nuanced view of model performance, especially in multi-class scenarios like our Iris dataset. These visualizations and metrics calculated provide a comprehensive view of the model’s performance and the dataset’s structure, aiding in better understanding and interpretation of the results.\n\n\n\nCode\nfrom sklearn import metrics\ny_pred = knn.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1)))\nprint(\"Precision:\", metrics.precision_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1), average='weighted'))\nprint(\"Recall:\", metrics.recall_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1), average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1), average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1), average='weighted'))\nprint(metrics.classification_report(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1), target_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']))\n\ncm = metrics.confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.title('Confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    y_score = knn.predict_proba(X_test)\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\ncolors = ['blue', 'red', 'green']\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic for Iris Dataset')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\nAccuracy: 0.9333333333333333\nPrecision: 0.9433333333333334\nRecall: 0.9333333333333333\nsensitivity: 0.9333333333333333\nf1 score: 0.9323323323323324\n                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        13\nIris-versicolor       0.85      1.00      0.92        17\n Iris-virginica       1.00      0.80      0.89        15\n\n       accuracy                           0.93        45\n      macro avg       0.95      0.93      0.94        45\n   weighted avg       0.94      0.93      0.93        45\n\n\n\n\n\n\n(a) Confusion Matrix\n\n\n\n\n\n\n\n(b) RoC Curve\n\n\n\nFigure 1: KNN Results"
  },
  {
    "objectID": "posts/Probability-and-Random-Variables/index.html",
    "href": "posts/Probability-and-Random-Variables/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In the digital age, spam has become an unavoidable nuisance, cluttering inboxes and consuming valuable time. However, machine learning offers a powerful tool in the fight against spam. In this blog, we’ll explore how the Naive Bayes classifier, a simple yet effective machine learning algorithm, can be utilized for spam detection. We will also include a Python code example with data visualization using an external dataset.\n\n\nNaive Bayes classifiers work on the principle of conditional probability, as derived from Bayes’ Theorem. In the context of spam detection, it calculates the probability of an email being spam based on the frequency of words it contains. Despite its simplicity, Naive Bayes can be incredibly effective in distinguishing spam from non-spam (ham) emails.\n\n\n\nTo demonstrate this, we’ll use a publicly available dataset of emails, pre-labeled as spam or ham.\n\n\nFirst, we’ll import the necessary libraries and load our dataset:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# Load the dataset\nemail_data = pd.read_csv('spam_ham_dataset.csv')\n\n\n\n\n\nWe’ll preprocess the data by converting the email texts into a format that can be used by our machine learning model:\n\n\nCode\n# Splitting the dataset into features and labels\nX = email_data['text']\ny = email_data['label'] # 1 for spam, 0 for ham\n\n# Convert text data into numerical vectors\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\n\nNow, we’ll train a Naive Bayes classifier on our data:\n\n\nCode\n# Training the Naive Bayes model\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n\n\n\nWe evaluate the performance of our model on the test set:\n\n\nCode\n# Predictions\npredictions = model.predict(X_test)\n\n# Evaluation\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\n\n\n              precision    recall  f1-score   support\n\n         ham       0.98      0.98      0.98      1121\n        spam       0.96      0.96      0.96       431\n\n    accuracy                           0.98      1552\n   macro avg       0.97      0.97      0.97      1552\nweighted avg       0.98      0.98      0.98      1552\n\n[[1103   18]\n [  19  412]]\n\n\n\n\n\nA confusion matrix and ROC Curve can be a great way to visualize the performance of our classifier as shown in Figure 1:\n\n\n\nCode\n# Visualizing the confusion matrix\ncm = confusion_matrix(y_test, predictions)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\nplt.title('Confusion Matrix for Spam Detection')\nplt.show()\n\n# Ensure that y is in a binary numerical format\ny = email_data['label'].replace({'ham': 0, 'spam': 1})\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nfrom sklearn.metrics import roc_curve, auc\n\n# Calculate the probabilities of each class\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Compute ROC curve and ROC area\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\n# Plotting ROC Curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n(a) Confusion Matrix\n\n\n\n\n\n\n\n(b) RoC Curve\n\n\n\nFigure 1: Naive Bayes Resullts\n\n\n\n\n\nThe confusion matrix provides a visual summary of how well our model performed. The matrix’s diagonal shows the number of correct predictions (true positives and true negatives), while off-diagonal elements are those that were incorrectly classified.\n\n\n\n\nNaive Bayes, with its assumption of feature independence and probability-based approach, is particularly well-suited for text classification tasks like spam detection. Its efficiency and ease of implementation make it an excellent choice for quickly deploying a spam detection system.\nThough simple, this example illustrates the power of machine learning in automating tasks that were traditionally handled manually. By training a machine learning model on examples of spam and non-spam emails, we can effectively filter out unwanted messages, saving time and improving our digital experience."
  },
  {
    "objectID": "posts/Probability-and-Random-Variables/index.html#the-power-of-naive-bayes-in-spam-detection",
    "href": "posts/Probability-and-Random-Variables/index.html#the-power-of-naive-bayes-in-spam-detection",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Naive Bayes classifiers work on the principle of conditional probability, as derived from Bayes’ Theorem. In the context of spam detection, it calculates the probability of an email being spam based on the frequency of words it contains. Despite its simplicity, Naive Bayes can be incredibly effective in distinguishing spam from non-spam (ham) emails."
  },
  {
    "objectID": "posts/Probability-and-Random-Variables/index.html#real-world-application-spam-detection-in-emails",
    "href": "posts/Probability-and-Random-Variables/index.html#real-world-application-spam-detection-in-emails",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "To demonstrate this, we’ll use a publicly available dataset of emails, pre-labeled as spam or ham.\n\n\nFirst, we’ll import the necessary libraries and load our dataset:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# Load the dataset\nemail_data = pd.read_csv('spam_ham_dataset.csv')\n\n\n\n\n\nWe’ll preprocess the data by converting the email texts into a format that can be used by our machine learning model:\n\n\nCode\n# Splitting the dataset into features and labels\nX = email_data['text']\ny = email_data['label'] # 1 for spam, 0 for ham\n\n# Convert text data into numerical vectors\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\n\nNow, we’ll train a Naive Bayes classifier on our data:\n\n\nCode\n# Training the Naive Bayes model\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n\n\n\nWe evaluate the performance of our model on the test set:\n\n\nCode\n# Predictions\npredictions = model.predict(X_test)\n\n# Evaluation\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\n\n\n              precision    recall  f1-score   support\n\n         ham       0.98      0.98      0.98      1121\n        spam       0.96      0.96      0.96       431\n\n    accuracy                           0.98      1552\n   macro avg       0.97      0.97      0.97      1552\nweighted avg       0.98      0.98      0.98      1552\n\n[[1103   18]\n [  19  412]]\n\n\n\n\n\nA confusion matrix and ROC Curve can be a great way to visualize the performance of our classifier as shown in Figure 1:\n\n\n\nCode\n# Visualizing the confusion matrix\ncm = confusion_matrix(y_test, predictions)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\nplt.title('Confusion Matrix for Spam Detection')\nplt.show()\n\n# Ensure that y is in a binary numerical format\ny = email_data['label'].replace({'ham': 0, 'spam': 1})\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nfrom sklearn.metrics import roc_curve, auc\n\n# Calculate the probabilities of each class\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Compute ROC curve and ROC area\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\n# Plotting ROC Curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n(a) Confusion Matrix\n\n\n\n\n\n\n\n(b) RoC Curve\n\n\n\nFigure 1: Naive Bayes Resullts\n\n\n\n\n\nThe confusion matrix provides a visual summary of how well our model performed. The matrix’s diagonal shows the number of correct predictions (true positives and true negatives), while off-diagonal elements are those that were incorrectly classified."
  },
  {
    "objectID": "posts/Probability-and-Random-Variables/index.html#conclusion",
    "href": "posts/Probability-and-Random-Variables/index.html#conclusion",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Naive Bayes, with its assumption of feature independence and probability-based approach, is particularly well-suited for text classification tasks like spam detection. Its efficiency and ease of implementation make it an excellent choice for quickly deploying a spam detection system.\nThough simple, this example illustrates the power of machine learning in automating tasks that were traditionally handled manually. By training a machine learning model on examples of spam and non-spam emails, we can effectively filter out unwanted messages, saving time and improving our digital experience."
  },
  {
    "objectID": "posts/Probability-and-Random-Variables/index.html#understanding-random-variables-in-test-scores",
    "href": "posts/Probability-and-Random-Variables/index.html#understanding-random-variables-in-test-scores",
    "title": "Probability Theory and Random Variables",
    "section": "Understanding Random Variables in Test Scores",
    "text": "Understanding Random Variables in Test Scores\nTest scores are a classic example of a continuous random variable. They can range over a continuum of values and, in many cases, follow a normal distribution. This means most students score around the average, with fewer students scoring very high or very low."
  },
  {
    "objectID": "posts/Probability-and-Random-Variables/index.html#real-world-scenario-exam-score-analysis",
    "href": "posts/Probability-and-Random-Variables/index.html#real-world-scenario-exam-score-analysis",
    "title": "Probability Theory and Random Variables",
    "section": "Real-World Scenario: Exam Score Analysis",
    "text": "Real-World Scenario: Exam Score Analysis\nLet’s say we have a dataset of student scores from a recent exam. Our goal is to visualize these scores and understand their distribution.\n\nStep 1: Setting Up the Python Environment\nFirst, we import the necessary libraries for our analysis:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nStep 2: Loading and Preparing the Dataset\nAssuming we have a dataset student_scores.csv that includes scores of students, we load it into a DataFrame:\n\n\nCode\n# Load the dataset\nscores_df = pd.read_csv('Student_marks.csv')\n\n\n\n\nStep 3: Data Visualization with Histogram\nWe’ll use a histogram to visualize the distribution of exam scores, as seen in Figure 2. A histogram is a great way to see the frequency of score ranges, and with scores typically following a normal distribution, we expect to see a bell-shaped curve.\n\n\nCode\n# Plotting the histogram\nplt.figure(figsize=(10, 6))\nsns.distplot(scores_df['Marks'], kde=True, color='green')\nplt.title('Distribution of Student Exam Scores')\nplt.xlabel('Score')\nplt.ylabel('Number of Students')\nplt.show()\n\n\n/var/folders/d2/vlmks3tx4jx2bk0gvbw50rr40000gn/T/ipykernel_36502/809933741.py:3: UserWarning:\n\n\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n\n\n\n\n\n\nFigure 2: Distribution of Student Exam Scores\n\n\n\n\n\n\nStep 4: Interpreting the Distribution\nThe histogram will likely display the bell-shaped curve of the normal distribution. This curve shows us how the test scores are spread out. Most students will score around the mean (the peak of the bell), with fewer students scoring significantly higher or lower."
  },
  {
    "objectID": "posts/Probability-and-Random-Variables/index.html#conclusion-1",
    "href": "posts/Probability-and-Random-Variables/index.html#conclusion-1",
    "title": "Probability Theory and Random Variables",
    "section": "Conclusion",
    "text": "Conclusion\nThe analysis of student test scores using the concept of random variables and probability theory provides valuable insights into student performance. It helps in identifying how scores are distributed, the average performance, and the variability in scores. Such analysis is crucial for educational institutions to evaluate the effectiveness of their teaching methods, curriculum design, and overall student understanding. Understanding the distribution of test scores also helps in setting realistic benchmarks and expectations, both for educators and students. It underscores the significance of data-driven approaches in educational settings, where decisions can be made based on empirical evidence rather than intuition."
  },
  {
    "objectID": "posts/Weathering-the-Storm-Anomaly-Detection-in-Meteorological-Data/index.html",
    "href": "posts/Weathering-the-Storm-Anomaly-Detection-in-Meteorological-Data/index.html",
    "title": "Weathering the Storm: Anomaly Detection in Meteorological Data",
    "section": "",
    "text": "Weather data is inherently complex and multifaceted, making it a fascinating subject for anomaly detection. From sudden temperature spikes to unexpected rainfall patterns, identifying unusual weather events is crucial for meteorologists and climate scientists. In this blog, we delve into the world of anomaly detection applied to weather data, employing machine learning techniques to uncover these rare but significant events."
  },
  {
    "objectID": "posts/Weathering-the-Storm-Anomaly-Detection-in-Meteorological-Data/index.html#why-isolation-forest",
    "href": "posts/Weathering-the-Storm-Anomaly-Detection-in-Meteorological-Data/index.html#why-isolation-forest",
    "title": "Weathering the Storm: Anomaly Detection in Meteorological Data",
    "section": "Why Isolation Forest?",
    "text": "Why Isolation Forest?\n\nEfficiency in High-Dimensional Data: Isolation Forest can effectively process datasets with multiple features, typical in weather data.\nAnomaly Isolation: Unlike many algorithms that try to model normal points, Isolation Forest isolates anomalies, making it well-suited for datasets where anomalies are unknown or rare.\nSpeed and Scalability: It offers fast execution, which is crucial for large datasets like those found in meteorology."
  },
  {
    "objectID": "posts/Weathering-the-Storm-Anomaly-Detection-in-Meteorological-Data/index.html#a-practical-example-detecting-anomalies-in-weather-data",
    "href": "posts/Weathering-the-Storm-Anomaly-Detection-in-Meteorological-Data/index.html#a-practical-example-detecting-anomalies-in-weather-data",
    "title": "Weathering the Storm: Anomaly Detection in Meteorological Data",
    "section": "A Practical Example: Detecting Anomalies in Weather Data",
    "text": "A Practical Example: Detecting Anomalies in Weather Data\nLet’s consider a dataset comprising daily weather observations, including temperature, humidity, wind speed, and precipitation levels.\n\nStep 1: Preparing the Python Environment\nWe start by setting up our environment with the necessary Python libraries:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nStep 2: Loading and Processing the Weather Data\nWe’ll use a public weather dataset for our analysis:\n\n\nCode\n# Load the dataset\nweather_data = pd.read_csv('weatherHistory.csv')\n\n# Selecting relevant features\nX = weather_data[['Temperature (C)', 'Humidity', 'Wind Speed (km/h)']]\n\n\n\n\nStep 3: Data Preprocessing\nScaling the data is important for algorithms like Isolation Forest:\n\n\nCode\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n\n\nStep 4: Applying Isolation Forest\nWe now apply the Isolation Forest algorithm to detect anomalies:\n\n\nCode\niso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\niso_forest.fit(X_scaled)\n\n# Predicting anomalies\nlabels = iso_forest.predict(X_scaled)\n\n\n\n\nStep 5: Visualizing Anomalies\nVisualizing the results helps in understanding the distribution of anomalies:\n\n\nCode\nweather_data['anomaly'] = labels\nweather_data['anomaly'] = weather_data['anomaly'].map({1: 0, -1: 1})\n\nanomalies = weather_data[weather_data['anomaly'] == 1]\n\nplt.figure(figsize=(10, 6))\nplt.scatter(weather_data['Temperature (C)'], weather_data['Wind Speed (km/h)'], c=weather_data['anomaly'], cmap='coolwarm')\nplt.title('Anomalies in Weather Data')\nplt.xlabel('Temperature')\nplt.ylabel('Humidity')\nplt.legend()\nplt.show()\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument."
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "",
    "text": "In the diverse landscape of machine learning, clustering stands out as a powerful tool for uncovering hidden structures in data. Unlike supervised learning, clustering is a form of unsupervised learning - it doesn’t rely on predefined labels. Instead, it groups the data based on similarity. This blog post delves into the world of clustering, using a real-world dataset as our guide. We will explore the K-Means algorithm, a popular clustering technique, complete with Python code and visualizations to illuminate our journey."
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#step-1-setting-up-the-environment",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#step-1-setting-up-the-environment",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "Step 1: Setting Up the Environment",
    "text": "Step 1: Setting Up the Environment\nFirst, we import necessary Python libraries:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#step-2-loading-and-preparing-the-dataset",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#step-2-loading-and-preparing-the-dataset",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "Step 2: Loading and Preparing the Dataset",
    "text": "Step 2: Loading and Preparing the Dataset\nWe’ll use a dataset that contains customer information like spending scores and annual incomes:\n\n\nCode\n# Load the dataset\ncustomer_data = pd.read_csv('Mall_customers.csv')\nX = customer_data[['Annual Income (k$)', 'Spending Score (1-100)']].values\n\n# Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)"
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#step-3-applying-k-means-clustering",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#step-3-applying-k-means-clustering",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "Step 3: Applying K-Means Clustering",
    "text": "Step 3: Applying K-Means Clustering\nWith our data prepared, we can now apply the K-Means algorithm:\n\n\nCode\n# Determine the optimal number of clusters\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n    kmeans.fit(X_scaled)\n    wcss.append(kmeans.inertia_)\n\n# Plot the Elbow Method graph\nplt.figure(figsize=(10,6))\nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n\n# Apply K-Means to the dataset\nkmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)\ny_kmeans = kmeans.fit_predict(X_scaled)\n\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning"
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#step-4-visualizing-the-clusters",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#step-4-visualizing-the-clusters",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "Step 4: Visualizing the Clusters",
    "text": "Step 4: Visualizing the Clusters\nFinally, we visualize the customer segments:\n\n\nCode\n# Visualizing the clusters\nplt.figure(figsize=(10, 6))\nplt.scatter(X_scaled[y_kmeans == 0, 0], X_scaled[y_kmeans == 0, 1], s=50, c='red', label='Cluster 1')\nplt.scatter(X_scaled[y_kmeans == 1, 0], X_scaled[y_kmeans == 1, 1], s=50, c='green', label='Cluster 1')\nplt.scatter(X_scaled[y_kmeans == 2, 0], X_scaled[y_kmeans == 2, 1], s=50, c='blue', label='Cluster 1')\nplt.scatter(X_scaled[y_kmeans == 3, 0], X_scaled[y_kmeans == 3, 1], s=50, c='pink', label='Cluster 1')\nplt.scatter(X_scaled[y_kmeans == 4, 0], X_scaled[y_kmeans == 4, 1], s=50, c='orange', label='Cluster 1')\n\n# ... repeat for other clusters ...\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label='Centroids')\nplt.title('Customer Segments')\nplt.xlabel('Annual Income (k$) [Standardized]')\nplt.ylabel('Spending Score (1-100) [Standardized]')\nplt.legend()\nplt.show()\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Assuming you have already applied K-Means and have y_kmeans\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Color map for different clusters\ncolors = ['blue', 'green', 'red', 'cyan', 'magenta']\n\n# Plotting each cluster\nfor i in range(kmeans.n_clusters):\n    ax.scatter(X_scaled[y_kmeans == i, 0], X_scaled[y_kmeans == i, 1], s=50, c=colors[i], label=f'Cluster {i+1}')\n\n# Plotting the centroids\nax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', depthshade=False, label='Centroids')\n\nax.set_title('3D view of Customer Segments')\nax.set_xlabel('Annual Income (Standardized)')\nax.set_ylabel('Spending Score (Standardized)')\nax.set_zlabel('Cluster Number')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThis 3D visualization provides an enriched perspective of the clustering results. It offers a unique view that can help in identifying how distinct the clusters are in a multi-dimensional space."
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#understanding-the-results",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#understanding-the-results",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "Understanding the Results",
    "text": "Understanding the Results\nThe clusters reveal distinct groups in our customer data, each possibly representing a different market segment. For instance, a cluster with high income and high spending scores might represent a premium segment."
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#kmeans-clustering",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#kmeans-clustering",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "KMeans Clustering",
    "text": "KMeans Clustering\nKMeans is an iterative algorithm that partitions data into K clusters. It works by assigning data points to the nearest cluster center and updating the center as the mean of the assigned points. This process continues until convergence."
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#dbscan-clustering",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#dbscan-clustering",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "DBSCAN Clustering",
    "text": "DBSCAN Clustering\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm. It groups together points that are closely packed and marks points that lie alone in low-density regions as outliers."
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#agglomerative-clustering",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#agglomerative-clustering",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "Agglomerative Clustering",
    "text": "Agglomerative Clustering\nAgglomerative Clustering is a hierarchical clustering technique that starts with each point as a separate cluster and merges the closest clusters iteratively until only one cluster remains."
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#kmeans-clustering-1",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#kmeans-clustering-1",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "KMeans Clustering",
    "text": "KMeans Clustering\nFigure 1 visualises the clusters created after kmeans clustering. Figure 1 (a) displays the correlation heat map to identify the most correlated features. It is observed that Annual Income (k$) and Spending Socre (1-100) are the most correlated features. Figure 1 (b) shows the elbow curve indicating that 4 clusters would be better to use and Figure 1 (c) shows the clusters after clustering.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\nimport pandas as pd\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv('Mall_Customers.csv')\nX = df[['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n\ncorr_matrix = np.corrcoef(X.values, rowvar=False)\nplt.imshow(corr_matrix, cmap='Wistia', interpolation='nearest')\nfor i in range(corr_matrix.shape[0]):\n    for j in range(corr_matrix.shape[1]):\n        plt.text(j, i, f'{corr_matrix[i, j]:.2f}', ha='center', va='center', color='black')\nplt.title('Heatmap for the Data', fontsize=20)\nplt.xticks(np.arange(corr_matrix.shape[0]), labels=['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)'], rotation=90)\nplt.yticks(np.arange(corr_matrix.shape[1]), labels=['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)'])\nplt.colorbar()\nplt.show()\n\nX = X.values\ndist = [] \nfor i in range(1,11): \n  km = KMeans(n_clusters=i, random_state=42).fit(X)\n  dist.append(km.inertia_)\nplt.plot(range(1,11), dist, marker='*')\nplt.xlabel('Number of cluster')\nplt.ylabel('Distortion')\nplt.show()\n\n# Applying KMeans algorithm\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\ncenters = kmeans.cluster_centers_\n\n# Visualizing clusters with different colors and a legend\nplt.scatter(X[y_kmeans == 0, 2], X[y_kmeans == 0, 3], c='yellow', s=50, cmap='viridis', label='Cluster1')\nplt.scatter(X[y_kmeans == 1, 2], X[y_kmeans == 1, 3], c='blue', s=50, cmap='viridis', label='Cluster2')\nplt.scatter(X[y_kmeans == 2, 2], X[y_kmeans == 2, 3], c='green', s=50, cmap='viridis', label='Cluster3')\nplt.scatter(X[y_kmeans == 3, 2], X[y_kmeans == 3, 3], c='violet', s=50, cmap='viridis', label='Cluster4')\nplt.scatter(centers[:, 2], centers[:, 3], c='red', s=200, alpha=0.5, label='Centroids')\nplt.legend()\nplt.show()\n\nprint('Silhouette Score: ',metrics.silhouette_score(X, km.labels_, metric='euclidean'))\n\n\n\n\n\n(a) Correlation Heat Map to identify correlated features\n\n\n\n\n\n\n\n(b) Elbow Curve\n\n\n\n\n\n\n\n(c) Cluster Visualization\n\n\n\n\nSilhouette Score:  0.3724780422340438\n\nFigure 1: KMeans Clustering"
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#dbscan-clustering-1",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#dbscan-clustering-1",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "DBSCAN Clustering",
    "text": "DBSCAN Clustering\nFigure 2 visualises the clusters created after DBScan\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import DBSCAN\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Generate sample data\nX, _ = make_moons(n_samples=1000, noise=0.05)\n\n# Apply DBSCAN algorithm\ndbscan = DBSCAN(eps=0.1, min_samples=5)\ny_pred = dbscan.fit_predict(X)\n\n# Visualize Clusters\nplt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], c='lightblue', marker='o', edgecolor='black', label='Cluster 1')\nplt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], c='lightgreen', marker='s', edgecolor='black', label='Cluster 2')\nplt.legend()\nplt.show()\n\n\n\n\n\nFigure 2: DBSCAN Clusters Created"
  },
  {
    "objectID": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#agglomerative-clustering-1",
    "href": "posts/A-Deep-Dive-into-Data-Grouping-Techniques/index.html#agglomerative-clustering-1",
    "title": "A Deep Dive into Data Grouping Techniques",
    "section": "Agglomerative Clustering",
    "text": "Agglomerative Clustering\nFigure 3 visualises the dendogram created after aglomerative clustering on the mall customer dataset\n\n\nCode\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.cluster import AgglomerativeClustering\nimport pandas as pd\n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx &lt; n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack(\n        [model.children_, model.distances_, counts]\n    ).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n\n\ndf = pd.read_csv('Mall_Customers.csv')\nX = df[['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)']].values\n\n# setting distance_threshold=0 ensures we compute the full tree.\nmodel = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n\nmodel = model.fit(X)\nplt.title(\"Hierarchical Clustering Dendrogram\")\n# plot the top three levels of the dendrogram\nplot_dendrogram(model, truncate_mode=\"level\", p=3)\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\nplt.show()\n\n\n\n\n\nFigure 3: Agglomerative Clustering Dendogram"
  },
  {
    "objectID": "posts/Navigating-Linear-and-Non-Linear-Relationships-with-Machine-Learning/index.html",
    "href": "posts/Navigating-Linear-and-Non-Linear-Relationships-with-Machine-Learning/index.html",
    "title": "Navigating Linear and Non-Linear Relationships with Machine Learning",
    "section": "",
    "text": "Understanding the relationship between variables is a cornerstone of data analysis. In the realm of machine learning, linear and non-linear regression models are pivotal tools for deciphering these relationships. In this blog post, we’ll explore both linear and non-linear regression, complete with a practical example using a real-world dataset. We will implement these concepts in Python and use data visualization to enhance our understanding."
  },
  {
    "objectID": "posts/Navigating-Linear-and-Non-Linear-Relationships-with-Machine-Learning/index.html#real-world-example-housing-prices",
    "href": "posts/Navigating-Linear-and-Non-Linear-Relationships-with-Machine-Learning/index.html#real-world-example-housing-prices",
    "title": "Navigating Linear and Non-Linear Relationships with Machine Learning",
    "section": "Real-World Example: Housing Prices",
    "text": "Real-World Example: Housing Prices\nLet’s consider a scenario where we predict housing prices based on various features like size, number of rooms, and age of the house. We’ll use a dataset containing these features and corresponding house prices.\n\nStep 1: Import Libraries and Load Data\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\n# Load dataset\nhousing_data = pd.read_csv('Housing.csv')\n\n\n\n\nStep 2: Preparing the Data\n\n\nCode\nX = housing_data[['area']]\ny = housing_data['price']\n# Initialize the Random Forest Regressor\nfrom sklearn.preprocessing import LabelEncoder\n\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\nhousing_data['mainroad'] = labelencoder.fit_transform(housing_data['mainroad'])\nhousing_data['guestroom'] = labelencoder.fit_transform(housing_data['guestroom'])\nhousing_data['hotwaterheating'] = labelencoder.fit_transform(housing_data['hotwaterheating'])\nhousing_data['parking'] = labelencoder.fit_transform(housing_data['parking'])\nhousing_data['airconditioning'] = labelencoder.fit_transform(housing_data['airconditioning'])\nhousing_data['furnishingstatus'] = labelencoder.fit_transform(housing_data['furnishingstatus'])\nhousing_data['basement'] = labelencoder.fit_transform(housing_data['basement'])\nhousing_data['prefarea'] = labelencoder.fit_transform(housing_data['prefarea'])\n\n\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n\n\n\nStep 3: Linear Regression Model\n\n\nCode\n# Initialize and train the model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n\n# Predicting the prices\ny_pred = linear_model.predict(X_test)\n\n\n\n\nStep 4: Visualization\nFigure 1 shows the plot of the actual and predicted housing prices\n\n\nCode\nplt.figure(figsize=(10,6))\nplt.scatter(X_test['area'], y_test, color='blue', label='Actual')  # Actual values\nplt.scatter(X_test['area'], y_pred, color='red', label='Predicted')\nplt.plot(X_test['area'], y_pred, color='black', label='Predicted Line')\n# plt.scatter(X, y)\nplt.xlabel('Area')\nplt.ylabel('Price')\nplt.title('Actual vs Predicted Housing Prices')\nplt.legend()\nplt.show()\n\n\n\n\n\nFigure 1: Actual and Predicted Prices\n\n\n\n\nLinear and non-linear regression models are powerful tools in machine learning for understanding and predicting relationships between variables. While linear models offer simplicity and are easy to interpret, non-linear models capture more complex patterns and can provide more accurate predictions in certain scenarios. In our examples, linear regression helped us understand housing prices and their variation with the area of the houses"
  },
  {
    "objectID": "posts/Navigating-Linear-and-Non-Linear-Relationships-with-Machine-Learning/index.html#random-forest-a-non-linear-powerhouse",
    "href": "posts/Navigating-Linear-and-Non-Linear-Relationships-with-Machine-Learning/index.html#random-forest-a-non-linear-powerhouse",
    "title": "Navigating Linear and Non-Linear Relationships with Machine Learning",
    "section": "Random Forest: A Non-Linear Powerhouse",
    "text": "Random Forest: A Non-Linear Powerhouse\nRandom Forest is an ensemble learning method, primarily used for classification and regression. It works by constructing a multitude of decision trees at training time and outputting the mean prediction of the individual trees for regression tasks. This method is highly effective due to its ability to capture complex non-linear relationships in data."
  },
  {
    "objectID": "posts/Navigating-Linear-and-Non-Linear-Relationships-with-Machine-Learning/index.html#predicting-housing-prices-with-random-forest",
    "href": "posts/Navigating-Linear-and-Non-Linear-Relationships-with-Machine-Learning/index.html#predicting-housing-prices-with-random-forest",
    "title": "Navigating Linear and Non-Linear Relationships with Machine Learning",
    "section": "Predicting Housing Prices with Random Forest",
    "text": "Predicting Housing Prices with Random Forest\nTo demonstrate the Random Forest in action, let’s consider a housing dataset that includes various features like house size, number of rooms, age, location, and other relevant factors, along with the house prices.\n\nStep 1: Setting Up the Environment\nFirst, let’s import the necessary Python libraries and load our dataset:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\n\n\nStep 2: Preparing the Data\nWe’ll select our features and target variable (house price) and split the data into training and testing sets:\n\n\nCode\nX = housing_data.drop('price', axis=1)\ny = housing_data['price']\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\nStep 3: Building and Training the Random Forest Model\nNow, we’ll create and train our Random Forest regressor:\n\n\nCode\nrf_regressor = RandomForestRegressor(n_estimators=1000, random_state=42)\n\n# Train the model\nrf_regressor.fit(X_train, y_train)\n\n\nRandomForestRegressor(n_estimators=1000, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(n_estimators=1000, random_state=42)\n\n\n\n\nStep 4: Making Predictions and Evaluating the Model\nWe use our trained model to predict house prices and evaluate its performance:\n\n\nCode\n# Making predictions\ny_pred = rf_regressor.predict(X_test)\n\n# Evaluating the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f'Mean Squared Error: {mse}')\n\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint(f'Co-efficient of regression: {r2}')\n\n\nMean Squared Error: 1821573390511.917\nCo-efficient of regression: 0.577006987790502\n\n\n\n\nStep 5: Visualizing the Results\nA scatter plot (Figure 2) can help visualize how well our predicted values compare against the actual values:\n\n\nCode\nplt.figure(figsize=(10,6))\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Prices')\nplt.ylabel('Predicted Prices')\nplt.title('Actual vs Predicted Housing Prices')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)\nplt.show()\n\n\n\n\n\nFigure 2: Distribution of actual and predicted prices\n\n\n\n\nFor each prediction, calculate the standard deviation of the outputs from all individual trees in the forest. This gives a measure of how much variance there is in the predictions from the different trees. This plot will give you an idea of the spread of predictions for each data point, indicating where the model is more or less certain about its predictions."
  }
]