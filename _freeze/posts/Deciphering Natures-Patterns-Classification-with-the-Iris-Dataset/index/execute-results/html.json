{
  "hash": "caaa4a0ebdd47ead6d143fe1717e68f7",
  "result": {
    "markdown": "---\ntitle: 'Deciphering Nature''s Patterns: Classification with the Iris Dataset'\nimage: image.jpeg\nauthor: Sahana Bhaskar\ndate: '2023-11-07'\ncategories:\n  - classification\n  - ensemble\n  - supervised learning\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n---\n\nIn the vast and intricate domain of machine learning, classification tasks hold a place of high regard. One of the most celebrated datasets in this area is the Iris dataset, a cornerstone for budding data scientists and seasoned professionals alike. This blog post is dedicated to exploring the nuances of classifying Iris flowers using a specific machine learning technique, enriched with practical code and visual insights.\n\n#The Iris Dataset: A Botanical Puzzle\nThe Iris dataset, a hallmark in the world of machine learning, presents a fascinating challenge: to classify iris flowers into one of three species (Setosa, Versicolour, and Virginica) based on four features - sepal length, sepal width, petal length, and petal width. Each species, represented by 50 samples in the dataset, has its unique combination of these features. The goal is clear: to build a model that can accurately predict the species based on these physical measurements.\n\n# Unraveling the Dataset with Machine Learning\nOur journey through the Iris dataset involves several key stages, each integral to the process of machine learning.\n\n## Preparatory Steps: Setting Up the Python Environment\nThe journey begins in Python, a haven for data science enthusiasts, courtesy of its extensive libraries. We import essential packages like Pandas for data manipulation, NumPy for numerical calculations, and Seaborn and Matplotlib for visualization. Scikit-learn, a cornerstone library in Python for machine learning, brings in the necessary algorithms and tools:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, label_binarize\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.multiclass import OneVsRestClassifier\nimport seaborn as sns\n```\n:::\n\n\n## Diving into the Data: Loading and Preparing the Iris Dataset\nThe dataset is loaded, and we proceed to bifurcate it into training and testing sets. This split is critical to evaluate our model effectively, ensuring that it can generalize well to unseen data:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Load the dataset\niris = pd.read_csv('IRIS.csv')\niris = iris.sample(frac=1)\nX = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\ny = iris['species']\ny = label_binarize(y, classes=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n\nn_classes = y.shape[1]\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n```\n:::\n\n\n## The Heart of the Matter: Choosing the K-Nearest Neighbors\nAlgorithm\nFor our classification task, we select the K-Nearest Neighbors (KNN) algorithm. KNN is a simple yet powerful non-parametric algorithm used in classification and regression. It operates on a straightforward principle: the class of a data point is determined by the majority class among its k-nearest neighbors. The value of 'k' is a crucial hyperparameter and determines the number of neighbors to consider while making the classification decision.\nKNN is particularly appealing for its ease of understanding and implementation. It makes no underlying assumptions about the distribution of data, making it versatile across various datasets. In our case, the decision to choose KNN stems from its efficacy in handling small, clean datasets like the Iris.\n\n## Implementing the KNN Classifier\nWe use Scikit-learn's implementation of KNN, which involves initializing the classifier, fitting it to our training data, and then using it to make predictions:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nknn = OneVsRestClassifier(KNeighborsClassifier(n_neighbors= 3))\nknn.fit(X_train, y_train)\n\nknn.score(X_test,y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n0.9333333333333333\n```\n:::\n:::\n\n\n## Evaluating and Visualizing the Model's Performance\nThe final step in our journey is to evaluate and visualize the model's performance. A confusion matrix provides a straightforward way to visualize the accuracy of the classifier, showing the correct and incorrect predictions across different classes. .The confusion matrix (cm) is calculated from the predictions (y_pred) and the true labels (y_test). The ROC curve, in particular, provides a nuanced view of model performance, especially in multi-class scenarios like our Iris dataset. These visualizations and metrics calculated provide a comprehensive view of the model's performance and the dataset's structure, aiding in better understanding and interpretation of the results.\n\n::: {#fig-results .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn import metrics\ny_pred = knn.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1)))\nprint(\"Precision:\", metrics.precision_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1), average='weighted'))\nprint(\"Recall:\", metrics.recall_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1), average='weighted'))\nprint(\"sensitivity:\", metrics.recall_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1), average='weighted'))\nprint(\"f1 score:\", metrics.f1_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1), average='weighted'))\nprint(metrics.classification_report(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1), target_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']))\n\ncm = metrics.confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.title('Confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    y_score = knn.predict_proba(X_test)\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\ncolors = ['blue', 'red', 'green']\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic for Iris Dataset')\nplt.legend(loc=\"lower right\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9333333333333333\nPrecision: 0.9433333333333334\nRecall: 0.9333333333333333\nsensitivity: 0.9333333333333333\nf1 score: 0.9323323323323324\n                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        13\nIris-versicolor       0.85      1.00      0.92        17\n Iris-virginica       1.00      0.80      0.89        15\n\n       accuracy                           0.93        45\n      macro avg       0.95      0.93      0.94        45\n   weighted avg       0.94      0.93      0.93        45\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Confusion Matrix](index_files/figure-html/fig-results-output-2.png){#fig-results-1 width=547 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![RoC Curve](index_files/figure-html/fig-results-output-3.png){#fig-results-2 width=599 height=449}\n:::\n\nKNN Results\n:::\n\n\n# Concluding Thoughts\nThrough the lens of the Iris dataset, we've navigated the fundamentals of classification in machine learning. The K-Nearest Neighbors algorithm, with its simplicity and effectiveness, serves as an excellent starting point for understanding classification tasks. The journey from raw data to insightful visualizations encapsulates the essence of machine learning: transforming data into knowledge.\nAs you embark on your machine learning adventures, remember that each dataset has a story to tell, and it's through algorithms like KNN that we can uncover these hidden narratives. Whether it's the petals of an Iris flower or the more complex datasets of the real world, the principles of classification remain a steadfast guide\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}