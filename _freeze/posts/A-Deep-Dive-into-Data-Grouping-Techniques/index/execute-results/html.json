{
  "hash": "ab12559641a9694e2b27b6bce1cd5258",
  "result": {
    "markdown": "---\ntitle: A Deep Dive into Data Grouping Techniques\nimage: image.png\nauthor: Swapnil Singh\ndate: '2023-11-05'\ncategories:\n  - clustering\n  - unsupervised learning\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n---\n\nIn the diverse landscape of machine learning, clustering stands out as a powerful tool for uncovering hidden structures in data. Unlike supervised learning, clustering is a form of unsupervised learning - it doesnâ€™t rely on predefined labels. Instead, it groups the data based on similarity. This blog post delves into the world of clustering, using a real-world dataset as our guide. We will explore the K-Means algorithm, a popular clustering technique, complete with Python code and visualizations to illuminate our journey.\n\n# The Essence of Clustering\nClustering aims to segregate data into distinct groups or 'clusters,' such that data points in the same group are more similar to each other than those in other groups. It's extensively used in market segmentation, anomaly detection, search result grouping, and image segmentation.\n\n# The K-Means Algorithm: Simplifying Complexity\nK-Means is a simple yet powerful clustering algorithm. It partitions the data into K distinct, non-overlapping subgroups (clusters), with each data point belonging to the cluster with the nearest mean. The algorithm involves the following steps:\n\nInitialization: K initial 'centroids' (mean points) are randomly selected from the data points.\n\nAssignment: Each data point is assigned to its nearest centroid, based on the squared Euclidean distance.\n\nUpdate: The centroids are recalculated as the mean of all data points assigned to that cluster.\n\nRepeat: Steps 2 and 3 are repeated until the centroids no longer move significantly.\n\n# A Real-World Application: Customer Segmentation\nTo illustrate clustering in action, we'll use a customer dataset containing spending scores and annual incomes. Our goal is to segment customers into distinct groups based on these attributes.\n\n## Step 1: Setting Up the Environment\nFirst, we import necessary Python libraries:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n```\n:::\n\n\n## Step 2: Loading and Preparing the Dataset\nWe'll use a dataset that contains customer information like spending scores and annual incomes:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Load the dataset\ncustomer_data = pd.read_csv('Mall_customers.csv')\nX = customer_data[['Annual Income (k$)', 'Spending Score (1-100)']].values\n\n# Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n```\n:::\n\n\n## Step 3: Applying K-Means Clustering\nWith our data prepared, we can now apply the K-Means algorithm:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Determine the optimal number of clusters\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n    kmeans.fit(X_scaled)\n    wcss.append(kmeans.inertia_)\n\n# Plot the Elbow Method graph\nplt.figure(figsize=(10,6))\nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n\n# Apply K-Means to the dataset\nkmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)\ny_kmeans = kmeans.fit_predict(X_scaled)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=816 height=523}\n:::\n:::\n\n\n## Step 4: Visualizing the Clusters\nFinally, we visualize the customer segments:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Visualizing the clusters\nplt.figure(figsize=(10, 6))\nplt.scatter(X_scaled[y_kmeans == 0, 0], X_scaled[y_kmeans == 0, 1], s=50, c='red', label='Cluster 1')\nplt.scatter(X_scaled[y_kmeans == 1, 0], X_scaled[y_kmeans == 1, 1], s=50, c='green', label='Cluster 1')\nplt.scatter(X_scaled[y_kmeans == 2, 0], X_scaled[y_kmeans == 2, 1], s=50, c='blue', label='Cluster 1')\nplt.scatter(X_scaled[y_kmeans == 3, 0], X_scaled[y_kmeans == 3, 1], s=50, c='pink', label='Cluster 1')\nplt.scatter(X_scaled[y_kmeans == 4, 0], X_scaled[y_kmeans == 4, 1], s=50, c='orange', label='Cluster 1')\n\n# ... repeat for other clusters ...\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label='Centroids')\nplt.title('Customer Segments')\nplt.xlabel('Annual Income (k$) [Standardized]')\nplt.ylabel('Spending Score (1-100) [Standardized]')\nplt.legend()\nplt.show()\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Assuming you have already applied K-Means and have y_kmeans\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Color map for different clusters\ncolors = ['blue', 'green', 'red', 'cyan', 'magenta']\n\n# Plotting each cluster\nfor i in range(kmeans.n_clusters):\n    ax.scatter(X_scaled[y_kmeans == i, 0], X_scaled[y_kmeans == i, 1], s=50, c=colors[i], label=f'Cluster {i+1}')\n\n# Plotting the centroids\nax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', depthshade=False, label='Centroids')\n\nax.set_title('3D view of Customer Segments')\nax.set_xlabel('Annual Income (Standardized)')\nax.set_ylabel('Spending Score (Standardized)')\nax.set_zlabel('Cluster Number')\nax.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=823 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=552 height=557}\n:::\n:::\n\n\nThis 3D visualization provides an enriched perspective of the clustering results. It offers a unique view that can help in identifying how distinct the clusters are in a multi-dimensional space.\n\n## Understanding the Results\nThe clusters reveal distinct groups in our customer data, each possibly representing a different market segment. For instance, a cluster with high income and high spending scores might represent a premium segment.\n\n# Conclusion\nClustering, especially K-Means, offers a profound way to uncover hidden patterns and structures in data. While our example focused on customer segmentation, the principles of K-Means can be applied across various domains and datasets. It's a testament to the power of machine learning in transforming raw data into meaningful insights.\nRemember, the success of clustering depends on factors like the choice of the number of clusters and the preprocessing steps. It requires both an understanding of the algorithm and the context of the data.\n\n# Introduction to Clustering Algorithms\n\n## KMeans Clustering\n\nKMeans is an iterative algorithm that partitions data into K clusters. It works by assigning data points to the nearest cluster center and updating the center as the mean of the assigned points. This process continues until convergence.\n\n## DBSCAN Clustering\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm. It groups together points that are closely packed and marks points that lie alone in low-density regions as outliers.\n\n## Agglomerative Clustering\n\nAgglomerative Clustering is a hierarchical clustering technique that starts with each point as a separate cluster and merges the closest clusters iteratively until only one cluster remains.\n\n# Python Code Examples\n## KMeans Clustering\n@fig-kmean visualises the clusters created after kmeans clustering. @fig-kmean-1 displays the correlation heat map to identify the most correlated features. It is observed that Annual Income (k$) and Spending Socre (1-100) are the most correlated features. @fig-kmean-2 shows the elbow curve indicating that 4 clusters would be better to use and @fig-kmean-3 shows the clusters after clustering.\n\n::: {#fig-kmean .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\nimport pandas as pd\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv('Mall_Customers.csv')\nX = df[['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n\ncorr_matrix = np.corrcoef(X.values, rowvar=False)\nplt.imshow(corr_matrix, cmap='Wistia', interpolation='nearest')\nfor i in range(corr_matrix.shape[0]):\n    for j in range(corr_matrix.shape[1]):\n        plt.text(j, i, f'{corr_matrix[i, j]:.2f}', ha='center', va='center', color='black')\nplt.title('Heatmap for the Data', fontsize=20)\nplt.xticks(np.arange(corr_matrix.shape[0]), labels=['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)'], rotation=90)\nplt.yticks(np.arange(corr_matrix.shape[1]), labels=['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)'])\nplt.colorbar()\nplt.show()\n\nX = X.values\ndist = [] \nfor i in range(1,11): \n  km = KMeans(n_clusters=i, random_state=42).fit(X)\n  dist.append(km.inertia_)\nplt.plot(range(1,11), dist, marker='*')\nplt.xlabel('Number of cluster')\nplt.ylabel('Distortion')\nplt.show()\n\n# Applying KMeans algorithm\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\ncenters = kmeans.cluster_centers_\n\n# Visualizing clusters with different colors and a legend\nplt.scatter(X[y_kmeans == 0, 2], X[y_kmeans == 0, 3], c='yellow', s=50, cmap='viridis', label='Cluster1')\nplt.scatter(X[y_kmeans == 1, 2], X[y_kmeans == 1, 3], c='blue', s=50, cmap='viridis', label='Cluster2')\nplt.scatter(X[y_kmeans == 2, 2], X[y_kmeans == 2, 3], c='green', s=50, cmap='viridis', label='Cluster3')\nplt.scatter(X[y_kmeans == 3, 2], X[y_kmeans == 3, 3], c='violet', s=50, cmap='viridis', label='Cluster4')\nplt.scatter(centers[:, 2], centers[:, 3], c='red', s=200, alpha=0.5, label='Centroids')\nplt.legend()\nplt.show()\n\nprint('Silhouette Score: ',metrics.silhouette_score(X, km.labels_, metric='euclidean'))\n```\n\n::: {.cell-output .cell-output-display}\n![Correlation Heat Map to identify correlated features](index_files/figure-html/fig-kmean-output-1.png){#fig-kmean-1 width=641 height=584}\n:::\n\n::: {.cell-output .cell-output-display}\n![Elbow Curve](index_files/figure-html/fig-kmean-output-2.png){#fig-kmean-2 width=589 height=443}\n:::\n\n::: {.cell-output .cell-output-display}\n![Cluster Visualization](index_files/figure-html/fig-kmean-output-3.png){#fig-kmean-3 width=575 height=411}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette Score:  0.3724780422340438\n```\n:::\n\nKMeans Clustering\n:::\n\n\n## DBSCAN Clustering\n@fig-dbscan visualises the clusters created after DBScan\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import DBSCAN\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Generate sample data\nX, _ = make_moons(n_samples=1000, noise=0.05)\n\n# Apply DBSCAN algorithm\ndbscan = DBSCAN(eps=0.1, min_samples=5)\ny_pred = dbscan.fit_predict(X)\n\n# Visualize Clusters\nplt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], c='lightblue', marker='o', edgecolor='black', label='Cluster 1')\nplt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], c='lightgreen', marker='s', edgecolor='black', label='Cluster 2')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![DBSCAN Clusters Created](index_files/figure-html/fig-dbscan-output-1.png){#fig-dbscan width=590 height=411}\n:::\n:::\n\n\n## Agglomerative Clustering\n@fig-agglomerative visualises the dendogram created after aglomerative clustering on the mall customer dataset\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.cluster import AgglomerativeClustering\nimport pandas as pd\n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack(\n        [model.children_, model.distances_, counts]\n    ).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n\n\ndf = pd.read_csv('Mall_Customers.csv')\nX = df[['CustomerID','Age', 'Annual Income (k$)', 'Spending Score (1-100)']].values\n\n# setting distance_threshold=0 ensures we compute the full tree.\nmodel = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n\nmodel = model.fit(X)\nplt.title(\"Hierarchical Clustering Dendrogram\")\n# plot the top three levels of the dendrogram\nplot_dendrogram(model, truncate_mode=\"level\", p=3)\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Agglomerative Clustering Dendogram](index_files/figure-html/fig-agglomerative-output-1.png){#fig-agglomerative width=583 height=452}\n:::\n:::\n\n\n# Conclusion\n\nClustering is a powerful technique for exploring and understanding complex datasets. Each algorithm has its unique strengths and weaknesses, making them suitable for different types of data and applications. By understanding the nuances of each algorithm, you can apply them effectively to uncover hidden patterns and insights in your data.\n\nIn this blog, we explored KMeans, DBSCAN, and Agglomerative Clustering and provided Python code examples for each. We hope this overview helps you get started with clustering and inspires you to explore more complex applications and datasets.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}