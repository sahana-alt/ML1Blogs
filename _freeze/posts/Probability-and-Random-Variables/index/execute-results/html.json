{
  "hash": "25ff1993e77ad33ef401b7c5539f07f8",
  "result": {
    "markdown": "---\ntitle: Probability Theory and Random Variables\nimage: image.jpeg\nauthor: Sahana Bhaskar\ndate: '2023-11-04'\ncategories:\n  - random variable\n  - probability\n  - statistics\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n---\n\n#  Tackling Spam with Naive Bayes: A Practical Approach\n\nIn the digital age, spam has become an unavoidable nuisance, cluttering inboxes and consuming valuable time. However, machine learning offers a powerful tool in the fight against spam. In this blog, we'll explore how the Naive Bayes classifier, a simple yet effective machine learning algorithm, can be utilized for spam detection. We will also include a Python code example with data visualization using an external dataset.\n\n## The Power of Naive Bayes in Spam Detection\n\nNaive Bayes classifiers work on the principle of conditional probability, as derived from Bayes' Theorem. In the context of spam detection, it calculates the probability of an email being spam based on the frequency of words it contains. Despite its simplicity, Naive Bayes can be incredibly effective in distinguishing spam from non-spam (ham) emails.\n\n## Real-World Application: Spam Detection in Emails\nTo demonstrate this, we'll use a publicly available dataset of emails, pre-labeled as spam or ham.\n\n### Step 1: Setting Up the Environment\nFirst, we'll import the necessary libraries and load our dataset:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# Load the dataset\nemail_data = pd.read_csv('spam_ham_dataset.csv')\n```\n:::\n\n\n### Step 2: Preprocessing the Data\nWe'll preprocess the data by converting the email texts into a format that can be used by our machine learning model:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Splitting the dataset into features and labels\nX = email_data['text']\ny = email_data['label'] # 1 for spam, 0 for ham\n\n# Convert text data into numerical vectors\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n```\n:::\n\n\n### Step 3: Training the Naive Bayes Classifier\nNow, we'll train a Naive Bayes classifier on our data:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Training the Naive Bayes model\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n### Step 4: Model Evaluation\nWe evaluate the performance of our model on the test set:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Predictions\npredictions = model.predict(X_test)\n\n# Evaluation\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n         ham       0.98      0.98      0.98      1121\n        spam       0.96      0.96      0.96       431\n\n    accuracy                           0.98      1552\n   macro avg       0.97      0.97      0.97      1552\nweighted avg       0.98      0.98      0.98      1552\n\n[[1103   18]\n [  19  412]]\n```\n:::\n:::\n\n\n### Step 5: Visualizing the Results\nA confusion matrix and ROC Curve can be a great way to visualize the performance of our classifier as shown in @fig-results:\n\n::: {#fig-results .cell execution_count=5}\n``` {.python .cell-code}\n# Visualizing the confusion matrix\ncm = confusion_matrix(y_test, predictions)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\nplt.title('Confusion Matrix for Spam Detection')\nplt.show()\n\n# Ensure that y is in a binary numerical format\ny = email_data['label'].replace({'ham': 0, 'spam': 1})\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nfrom sklearn.metrics import roc_curve, auc\n\n# Calculate the probabilities of each class\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Compute ROC curve and ROC area\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\n# Plotting ROC Curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Confusion Matrix](index_files/figure-html/fig-results-output-1.png){#fig-results-1 width=564 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![RoC Curve](index_files/figure-html/fig-results-output-2.png){#fig-results-2 width=599 height=449}\n:::\n\nNaive Bayes Resullts\n:::\n\n\n### Understanding the Results\nThe confusion matrix provides a visual summary of how well our model performed. The matrix's diagonal shows the number of correct predictions (true positives and true negatives), while off-diagonal elements are those that were incorrectly classified.\n\n## Conclusion\nNaive Bayes, with its assumption of feature independence and probability-based approach, is particularly well-suited for text classification tasks like spam detection. Its efficiency and ease of implementation make it an excellent choice for quickly deploying a spam detection system.\n\nThough simple, this example illustrates the power of machine learning in automating tasks that were traditionally handled manually. By training a machine learning model on examples of spam and non-spam emails, we can effectively filter out unwanted messages, saving time and improving our digital experience.\n\n# Demystifying Test Scores: Analyzing Student Performance with Random Variables\n\nIn the educational landscape, understanding and analyzing student performance is vital for educators and policymakers. Test scores, a quintessential example of random variables in education, often follow a normal distribution – commonly known as the bell-shaped curve. In this blog post, we will explore how to analyze student test scores using probability theory and random variables. We'll illustrate this with Python code and include data visualization to provide a clearer picture of this distribution.\n\n## Understanding Random Variables in Test Scores\nTest scores are a classic example of a continuous random variable. They can range over a continuum of values and, in many cases, follow a normal distribution. This means most students score around the average, with fewer students scoring very high or very low.\n\n## Real-World Scenario: Exam Score Analysis\nLet's say we have a dataset of student scores from a recent exam. Our goal is to visualize these scores and understand their distribution.\n\n### Step 1: Setting Up the Python Environment\nFirst, we import the necessary libraries for our analysis:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n:::\n\n\n### Step 2: Loading and Preparing the Dataset\nAssuming we have a dataset student_scores.csv that includes scores of students, we load it into a DataFrame:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Load the dataset\nscores_df = pd.read_csv('Student_marks.csv')\n```\n:::\n\n\n### Step 3: Data Visualization with Histogram\nWe'll use a histogram to visualize the distribution of exam scores, as seen in @fig-plot. A histogram is a great way to see the frequency of score ranges, and with scores typically following a normal distribution, we expect to see a bell-shaped curve.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Plotting the histogram\nplt.figure(figsize=(10, 6))\nsns.distplot(scores_df['Marks'], kde=True, color='green')\nplt.title('Distribution of Student Exam Scores')\nplt.xlabel('Score')\nplt.ylabel('Number of Students')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/d2/vlmks3tx4jx2bk0gvbw50rr40000gn/T/ipykernel_36502/809933741.py:3: UserWarning:\n\n\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Distribution of Student Exam Scores](index_files/figure-html/fig-plot-output-2.png){#fig-plot width=829 height=523}\n:::\n:::\n\n\n### Step 4: Interpreting the Distribution\nThe histogram will likely display the bell-shaped curve of the normal distribution. This curve shows us how the test scores are spread out. Most students will score around the mean (the peak of the bell), with fewer students scoring significantly higher or lower.\n\n## Conclusion\nThe analysis of student test scores using the concept of random variables and probability theory provides valuable insights into student performance. It helps in identifying how scores are distributed, the average performance, and the variability in scores. Such analysis is crucial for educational institutions to evaluate the effectiveness of their teaching methods, curriculum design, and overall student understanding.\nUnderstanding the distribution of test scores also helps in setting realistic benchmarks and expectations, both for educators and students. It underscores the significance of data-driven approaches in educational settings, where decisions can be made based on empirical evidence rather than intuition.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}