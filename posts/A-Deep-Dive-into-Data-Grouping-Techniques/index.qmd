---
title: "A Deep Dive into Data Grouping Techniques"
image: image.webp
author: "Swapnil Singh"
date: "2023-11-30"
categories: [clustering, unsupervised learning]
format:
    html:
        code-fold: true
        code-tools: true
jupyter: python3
---

In the diverse landscape of machine learning, clustering stands out as a powerful tool for uncovering hidden structures in data. Unlike supervised learning, clustering is a form of unsupervised learning - it doesnâ€™t rely on predefined labels. Instead, it groups the data based on similarity. This blog post delves into the world of clustering, using a real-world dataset as our guide. We will explore the K-Means algorithm, a popular clustering technique, complete with Python code and visualizations to illuminate our journey.

# The Essence of Clustering
Clustering aims to segregate data into distinct groups or 'clusters,' such that data points in the same group are more similar to each other than those in other groups. It's extensively used in market segmentation, anomaly detection, search result grouping, and image segmentation.

# The K-Means Algorithm: Simplifying Complexity
K-Means is a simple yet powerful clustering algorithm. It partitions the data into K distinct, non-overlapping subgroups (clusters), with each data point belonging to the cluster with the nearest mean. The algorithm involves the following steps:

Initialization: K initial 'centroids' (mean points) are randomly selected from the data points.

Assignment: Each data point is assigned to its nearest centroid, based on the squared Euclidean distance.

Update: The centroids are recalculated as the mean of all data points assigned to that cluster.

Repeat: Steps 2 and 3 are repeated until the centroids no longer move significantly.

# A Real-World Application: Customer Segmentation
To illustrate clustering in action, we'll use a customer dataset containing spending scores and annual incomes. Our goal is to segment customers into distinct groups based on these attributes.

## Step 1: Setting Up the Environment
First, we import necessary Python libraries:
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
```

## Step 2: Loading and Preparing the Dataset
We'll use a dataset that contains customer information like spending scores and annual incomes:
```{python}
# Load the dataset
customer_data = pd.read_csv('Mall_customers.csv')
X = customer_data[['Annual Income (k$)', 'Spending Score (1-100)']].values

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

## Step 3: Applying K-Means Clustering
With our data prepared, we can now apply the K-Means algorithm, @fig-elbow helps to identify the value of K:
```{python}
#| label: fig-elbow
#| fig-cap: "Elbow Curve"
# Determine the optimal number of clusters
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method graph
plt.figure(figsize=(10,6))
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Apply K-Means to the dataset
kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)
y_kmeans = kmeans.fit_predict(X_scaled)
```

## Step 4: Visualizing the Clusters
Finally, we visualize the customer segments:
```{python}
#| label: fig-cluster
#| fig-cap: "Clustering Results"
#| fig-subcap:
#|  - 2D Clusters
#|  - 3D Clusters
# Visualizing the clusters
plt.figure(figsize=(10, 6))
plt.scatter(X_scaled[y_kmeans == 0, 0], X_scaled[y_kmeans == 0, 1], s=50, c='red', label='Cluster 1')
plt.scatter(X_scaled[y_kmeans == 1, 0], X_scaled[y_kmeans == 1, 1], s=50, c='green', label='Cluster 1')
plt.scatter(X_scaled[y_kmeans == 2, 0], X_scaled[y_kmeans == 2, 1], s=50, c='blue', label='Cluster 1')
plt.scatter(X_scaled[y_kmeans == 3, 0], X_scaled[y_kmeans == 3, 1], s=50, c='pink', label='Cluster 1')
plt.scatter(X_scaled[y_kmeans == 4, 0], X_scaled[y_kmeans == 4, 1], s=50, c='orange', label='Cluster 1')

# ... repeat for other clusters ...
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label='Centroids')
plt.title('Customer Segments')
plt.xlabel('Annual Income (k$) [Standardized]')
plt.ylabel('Spending Score (1-100) [Standardized]')
plt.legend()
plt.show()

from mpl_toolkits.mplot3d import Axes3D

# Assuming you have already applied K-Means and have y_kmeans
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Color map for different clusters
colors = ['blue', 'green', 'red', 'cyan', 'magenta']

# Plotting each cluster
for i in range(kmeans.n_clusters):
    ax.scatter(X_scaled[y_kmeans == i, 0], X_scaled[y_kmeans == i, 1], s=50, c=colors[i], label=f'Cluster {i+1}')

# Plotting the centroids
ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', depthshade=False, label='Centroids')

ax.set_title('3D view of Customer Segments')
ax.set_xlabel('Annual Income (Standardized)')
ax.set_ylabel('Spending Score (Standardized)')
ax.set_zlabel('Cluster Number')
ax.legend()
plt.show()
```

This 3D visualization provides an enriched perspective of the clustering results. It offers a unique view that can help in identifying how distinct the clusters are in a multi-dimensional space.

## Understanding the Results
The clusters reveal distinct groups in our customer data, each possibly representing a different market segment. For instance, a cluster with high income and high spending scores might represent a premium segment.

# Conclusion
Clustering, especially K-Means, offers a profound way to uncover hidden patterns and structures in data. While our example focused on customer segmentation, the principles of K-Means can be applied across various domains and datasets. It's a testament to the power of machine learning in transforming raw data into meaningful insights.
Remember, the success of clustering depends on factors like the choice of the number of clusters and the preprocessing steps. It requires both an understanding of the algorithm and the context of the data.